{"metadata": {"kernelspec": {"name": "0123-172222-coif2___python3", "language": "python", "display_name": "Jupyter (Python 3), running"}, "name": "IoT Modeling", "notebookId": 5558681, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.7.3", "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "Create dummy data with:\n- `device_id`: 10 different devices\n- `record_id`: 10k unique records\n- `feature_1`: a feature for model training\n- `feature_2`: a feature for model training\n- `feature_3`: a feature for model training\n- `label`: the variable we're trying to predict"}, {"execution_count": 6, "metadata": {"trusted": true}, "cell_type": "code", "source": "import pyspark.sql.functions as f\n\ndf = (spark.range(100*100)\n  .select(f.col(\"id\").alias(\"record_id\"), (f.col(\"id\") % 10).alias(\"device_id\"))\n  .withColumn(\"feature_1\", f.rand() * 1)\n  .withColumn(\"feature_2\", f.rand() * 2)\n  .withColumn(\"feature_3\", f.rand() * 3)\n  .withColumn(\"label\", (f.col(\"feature_1\") + f.col(\"feature_2\") + f.col(\"feature_3\")) + f.rand())\n)\n\ndf.show()", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": "<pre class=\"jp-command-status-text\">Command took 0.20 seconds -- by niall.turbitt@databricks.com at 02/13/2020, 04:13:07 PM UTC on Jupyter</pre>"}}, {"output_type": "display_data", "metadata": {}, "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "87aa07a26d8f41a4ab05aa75ff04b2c5", "version_major": 2, "version_minor": 0}, "text/plain": "Accordion(children=(VBox(),), layout=Layout(display='none'), selected_index=None)"}}, {"output_type": "stream", "text": "+---------+---------+--------------------+--------------------+-------------------+------------------+\n|record_id|device_id|           feature_1|           feature_2|          feature_3|             label|\n+---------+---------+--------------------+--------------------+-------------------+------------------+\n|        0|        0|  0.3798100624574545|  1.4202490062082653| 1.6781901282345544| 3.525612733862513|\n|        1|        1|  0.9107536883090278|   1.442838269067278|  2.611383485089366| 5.814245271889851|\n|        2|        2| 0.41590416052434676| 0.10418460693028986|0.22604878358315217|1.6008841116808505|\n|        3|        3|  0.5512047715185051|  1.8502137667712624| 2.7280610877991984| 5.679620353053467|\n|        4|        4|  0.6497210433084482|  1.4206298832076816| 0.7105026181927976| 3.736893714115734|\n|        5|        5|  0.9773319027898405| 0.12361450125920048| 1.3702032077472008|2.7353747472986503|\n|        6|        6|  0.4380566234523088|  1.3427775095169203|  1.073934398606622|3.1789364444138983|\n|        7|        7|  0.6747300377451944|0.003603516229448...| 0.3502879100341326|1.2348706091740567|\n|        8|        8|  0.9399612891655168| 0.30925512124676424|0.48299443107232276|2.6117111102659796|\n|        9|        9|  0.8174852640887844|   0.789030281310404| 2.4296288144841274| 4.371624401955663|\n|       10|        0| 0.12071776857809124|  1.4513535587933488| 0.9394496769277029|2.7603229961034055|\n|       11|        1| 0.03335268572716421|  1.8922879075144932| 2.7871334138252806| 5.688580695879129|\n|       12|        2|   0.711898226434747|   1.266877135830694|0.25451106413528857|2.5511034570269033|\n|       13|        3|  0.8301335640752872|  1.0764570439889862| 1.8387135418836191| 4.543259324025714|\n|       14|        4|  0.9923107126421146| 0.31113714190052466| 2.0472722101511573| 3.859724632068804|\n|       15|        5|  0.4990672665997946|  1.1781405429539205|0.07352826786684052| 2.182118924981075|\n|       16|        6|0.014132093930306344| 0.16850393452115364| 2.3604581626507937|2.6641644225798533|\n|       17|        7|  0.2317452118738741|   0.919627226674971| 2.4676565424944337|4.0522546981944485|\n|       18|        8|  0.9151544715936084|  1.3623328455681951| 1.9223221155315593| 4.596272796730099|\n|       19|        9| 0.47798012340769547|  1.7828256086801857|  0.279639241652442|3.4424694001770093|\n+---------+---------+--------------------+--------------------+-------------------+------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}, {"output_type": "stream", "text": "/databricks/python3/lib/python3.7/site-packages/ipywidgets/widgets/widget.py:410: DeprecationWarning: Passing unrecoginized arguments to super(IntProgress).__init__(step=1).\nobject.__init__() takes exactly one argument (the instance to initialize)\nThis is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n  super(Widget, self).__init__(**kwargs)\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Enable Apache Arrow"}, {"execution_count": 7, "metadata": {"trusted": true}, "cell_type": "code", "source": "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": "<pre class=\"jp-command-status-text\">Command took 0.01 seconds -- by niall.turbitt@databricks.com at 02/13/2020, 04:15:32 PM UTC on Jupyter</pre>"}}, {"output_type": "display_data", "metadata": {}, "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "929083463e174db392274f8e51852a68", "version_major": 2, "version_minor": 0}, "text/plain": "Accordion(children=(VBox(),), layout=Layout(display='none'), selected_index=None)"}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Define the return schema"}, {"execution_count": 8, "metadata": {"trusted": true}, "cell_type": "code", "source": "import pyspark.sql.types as t\n\ntrainReturnSchema = t.StructType([\n  t.StructField('device_id', t.IntegerType()),    # unique device ID\n  t.StructField('n_used', t.IntegerType()),       # number of records used in training\n  t.StructField('model_path', t.StringType()),    # path to the model for a given device\n  t.StructField('mse', t.FloatType())             # metric for model performance\n])", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": "<pre class=\"jp-command-status-text\">Command took 0.01 seconds -- by niall.turbitt@databricks.com at 02/13/2020, 04:15:34 PM UTC on Jupyter</pre>"}}, {"output_type": "display_data", "metadata": {}, "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "ecb243b12ac143bcb9ae3cd72131ac45", "version_major": 2, "version_minor": 0}, "text/plain": "Accordion(children=(VBox(),), layout=Layout(display='none'), selected_index=None)"}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Define a pandas UDF that takes all the data for a given device, train a model, saves it as a nested run, and returns a spark object with the above schema"}, {"execution_count": 11, "metadata": {"trusted": true}, "cell_type": "code", "source": "import mlflow\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n@f.pandas_udf(trainReturnSchema, functionType=f.PandasUDFType.GROUPED_MAP)\ndef train_model(df_pandas):\n    \"\"\"\n    Trains an sklearn model on grouped instances\n    \"\"\"\n    # Pull metadata\n    device_id = df_pandas['device_id'].iloc[0]\n    n_used = df_pandas.shape[0]\n    run_id = df_pandas['run_id'].iloc[0]\n  \n    # Train the model\n    input_columns = ['feature_1', 'feature_2', 'feature_3']\n    X = df_pandas[input_columns]\n    y = df_pandas['label']\n    rf = RandomForestRegressor()\n    rf.fit(X, y)\n\n    # Evaluate the model\n    predictions = rf.predict(X)\n    mse = mean_squared_error(y, predictions) # NOTE NO TRAIN/TEST SPLIT\n\n    # Log the results as a nested run\n    # Note that we need 2 with blocks due to the distributed nature of this action\n    with mlflow.start_run(run_id=run_id):\n        with mlflow.start_run(run_name=str(device_id), nested=True) as run:\n            mlflow.sklearn.log_model(rf, str(device_id))\n            mlflow.log_metric(\"mse\", mse)\n\n            artifact_uri = run.info.artifact_uri + \"/\" + str(device_id)\n            # Create a return pandas DataFrame that matches the schema above\n            returnDF = pd.DataFrame([[device_id, n_used, artifact_uri, mse]], \n            columns=[\"device_id\", \"n_used\", \"model_path\", \"mse\"])\n\n    return returnDF \n", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": "<pre class=\"jp-command-status-text\">Command took 0.30 seconds -- by niall.turbitt@databricks.com at 02/13/2020, 04:17:33 PM UTC on Jupyter</pre>"}}, {"output_type": "display_data", "metadata": {}, "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "24afdd602dbb4787b7ab75c0f71ad8a9", "version_major": 2, "version_minor": 0}, "text/plain": "Accordion(children=(VBox(),), layout=Layout(display='none'), selected_index=None)"}}]}, {"execution_count": 15, "metadata": {"trusted": true}, "cell_type": "code", "source": "with mlflow.start_run(run_name=\"Parent Run\") as run:\n    run_id = run.info.run_uuid\n    \n    print(run_id)\n    ", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": "<pre class=\"jp-command-status-text\">Command took 0.25 seconds -- by niall.turbitt@databricks.com at 02/13/2020, 04:24:02 PM UTC on Jupyter</pre>"}}, {"output_type": "display_data", "metadata": {}, "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "44a53ae638184c2caab9526e7b05d79a", "version_major": 2, "version_minor": 0}, "text/plain": "Accordion(children=(VBox(),), layout=Layout(display='none'), selected_index=None)"}}, {"output_type": "stream", "text": "4aa97073258b4726bb1381f3ce713a2a\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Apply the pandas UDF to grouped data"}, {"execution_count": 17, "metadata": {"trusted": true}, "cell_type": "code", "source": "import tempfile\n\nmlflow.set_experiment(f\"/Users/niall.turbitt@databricks/test_workspace_2\")\n\n# Create the parent run and add run_id to the workers so they can find the parent run\nwith mlflow.start_run(run_name=\"Parent Run\") as run:\n\n    run_id = run.info.run_uuid\n  \n  # This is doing most of the work\n    modelDirectoriesDF = df.withColumn(\"run_id\", f.lit(run_id)).groupby(\"device_id\").apply(train_model)\n  \n  # Log modelDirectoriesDF to the parent run using a temporary file\n    temp = tempfile.NamedTemporaryFile(prefix=\"modelDirectoriesDF-\", suffix=\".csv\")\n    temp_name = temp.name\n    try:\n        modelDirectoriesDF.toPandas().to_csv(temp_name, index=False)\n        mlflow.log_artifact(temp_name, \"modelDirectoriesDF.csv\")\n    finally:\n        temp.close() # Delete the temp file\n        \nmodelDirectoriesDF.show()", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": "<pre class=\"jp-command-status-text\">Command took 10.28 seconds -- by niall.turbitt@databricks.com at 02/13/2020, 04:36:21 PM UTC on Jupyter</pre>"}}, {"output_type": "display_data", "metadata": {}, "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b1fc471a4096403f9f1aa98e4b7d933f", "version_major": 2, "version_minor": 0}, "text/plain": "Accordion(children=(VBox(),), layout=Layout(display='none'), selected_index=None)"}}, {"output_type": "stream", "text": "INFO: '/Users/niall.turbitt@databricks/test_workspace_2' does not exist. Creating a new experiment\n", "name": "stdout"}, {"output_type": "stream", "text": "/databricks/python3/lib/python3.7/site-packages/ipywidgets/widgets/widget.py:410: DeprecationWarning: Passing unrecoginized arguments to super(IntProgress).__init__(step=1).\nobject.__init__() takes exactly one argument (the instance to initialize)\nThis is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n  super(Widget, self).__init__(**kwargs)\n/databricks/spark/python/pyspark/sql/dataframe.py:2195: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.fallback.enabled' does not have an effect on failures in the middle of computation.\n  An error occurred while calling o1769.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:358)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:67)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 858.0 failed 4 times, most recent failure: Lost task 45.3 in stage 858.0 (TID 3758, 10.0.228.150, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run 'd7cdceac941142b099cc3b44204c51de' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:116)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3440)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3409)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:628)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:624)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1172)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1166)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1$$anonfun$apply$1.apply(SocketAuthServer.scala:48)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:48)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:47)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:102)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run 'd7cdceac941142b099cc3b44204c51de' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:116)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\n", "name": "stderr"}, {"output_type": "error", "ename": "Py4JJavaError", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m<ipython-input-17-ad09b4512ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtemp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodelDirectoriesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"modelDirectoriesDF.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2177\u001b[0m                         \u001b[0m_check_dataframe_localize_timestamps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m                     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2179\u001b[0;31m                     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collectAsArrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2180\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m                         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_collectAsArrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrowStreamSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m                 \u001b[0mjsocket_auth_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Join serving thread and raise any exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1769.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:358)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:67)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 858.0 failed 4 times, most recent failure: Lost task 45.3 in stage 858.0 (TID 3758, 10.0.228.150, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run 'd7cdceac941142b099cc3b44204c51de' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:116)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3440)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3409)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:628)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:624)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1172)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1166)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1$$anonfun$apply$1.apply(SocketAuthServer.scala:48)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:48)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:47)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:102)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run 'd7cdceac941142b099cc3b44204c51de' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:116)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"], "evalue": "An error occurred while calling o1769.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:358)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:67)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 858.0 failed 4 times, most recent failure: Lost task 45.3 in stage 858.0 (TID 3758, 10.0.228.150, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run 'd7cdceac941142b099cc3b44204c51de' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:116)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3440)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3409)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:628)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:624)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1172)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1166)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1$$anonfun$apply$1.apply(SocketAuthServer.scala:48)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:48)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:47)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:102)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run 'd7cdceac941142b099cc3b44204c51de' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:116)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20$$anonfun$apply$21.apply(Dataset.scala:3442)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Combine the orignal data to the new DataFrame so we can use `model_path`"}, {"execution_count": 13, "metadata": {"trusted": true}, "cell_type": "code", "source": "combinedDF = (df\n  .join(modelDirectoriesDF, on=\"device_id\", how=\"left\")\n)\n\ncombinedDF.show()", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": "<pre class=\"jp-command-status-text\">Command took 10.54 seconds -- by niall.turbitt@databricks.com at 02/13/2020, 04:20:07 PM UTC on Jupyter</pre>"}}, {"output_type": "display_data", "metadata": {}, "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "8f28c7acf2694cf48d70508924844b17", "version_major": 2, "version_minor": 0}, "text/plain": "Accordion(children=(VBox(),), layout=Layout(display='none'), selected_index=None)"}}, {"output_type": "error", "ename": "Py4JJavaError", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m<ipython-input-13-39780a6804a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcombinedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1465.showString.\n: org.apache.spark.SparkException: Exception thrown in Future.get: \n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:195)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:168)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:156)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:134)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:111)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:548)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:602)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:77)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:57)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectResult(Dataset.scala:2890)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3508)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3492)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3487)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:171)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2833)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:266)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:303)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 49 in stage 840.0 failed 4 times, most recent failure: Lost task 49.3 in stage 840.0 (TID 3189, 10.0.228.150, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run '6c9fd0ce3c6744339db10a64cfd5487c' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:151)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:150)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:182)\n\t... 62 more\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 49 in stage 840.0 failed 4 times, most recent failure: Lost task 49.3 in stage 840.0 (TID 3189, 10.0.228.150, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run '6c9fd0ce3c6744339db10a64cfd5487c' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:151)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:150)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2378)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:245)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:280)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:480)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:325)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1$$anonfun$call$1.apply(BroadcastExchangeExec.scala:90)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1$$anonfun$call$1.apply(BroadcastExchangeExec.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:196)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:193)\n\tat org.apache.spark.sql.execution.SQLExecution$.dbrWithExecutionId(SQLExecution.scala:216)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.call(BroadcastExchangeExec.scala:77)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.call(BroadcastExchangeExec.scala:73)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:97)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run '6c9fd0ce3c6744339db10a64cfd5487c' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:151)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:150)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\t... 3 more\n"], "evalue": "An error occurred while calling o1465.showString.\n: org.apache.spark.SparkException: Exception thrown in Future.get: \n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:195)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:168)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:156)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:134)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:111)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:548)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:602)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:77)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:57)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectResult(Dataset.scala:2890)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3508)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3492)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3487)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:171)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2833)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:266)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:303)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 49 in stage 840.0 failed 4 times, most recent failure: Lost task 49.3 in stage 840.0 (TID 3189, 10.0.228.150, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run '6c9fd0ce3c6744339db10a64cfd5487c' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:151)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:150)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:182)\n\t... 62 more\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 49 in stage 840.0 failed 4 times, most recent failure: Lost task 49.3 in stage 840.0 (TID 3189, 10.0.228.150, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run '6c9fd0ce3c6744339db10a64cfd5487c' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:151)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:150)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2378)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:245)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:280)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:480)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:325)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1$$anonfun$call$1.apply(BroadcastExchangeExec.scala:90)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1$$anonfun$call$1.apply(BroadcastExchangeExec.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:196)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:193)\n\tat org.apache.spark.sql.execution.SQLExecution$.dbrWithExecutionId(SQLExecution.scala:216)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.call(BroadcastExchangeExec.scala:77)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.call(BroadcastExchangeExec.scala:73)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply$mcV$sp(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$anonfun$run$1.apply(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$class.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:97)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:100)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 480, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 408, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 215, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 398, in init_stream_yield_batches\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/worker.py\", line 136, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/databricks/spark/python/pyspark/worker.py\", line 121, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/databricks/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-11-9662324b31f3>\", line 30, in train_model\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/fluent.py\", line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/client.py\", line 92, in get_run\n    return self._tracking_client.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py\", line 48, in get_run\n    return self.store.get_run(run_id)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/store/tracking/rest_store.py\", line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 137, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n  File \"/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py\", line 103, in verify_rest_response\n    raise RestException(json.loads(response.text))\nmlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: Run '6c9fd0ce3c6744339db10a64cfd5487c' not found.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:194)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:144)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:151)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$1.apply(Collector.scala:150)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.SparkContext$$anonfun$41.apply(SparkContext.scala:2377)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\t... 3 more\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Define a pandas UDF to apply the model.  **Note this only needs 1 read from DBFS per device**"}, {"execution_count": 14, "metadata": {}, "cell_type": "code", "source": "applyReturnSchema = t.StructType([\n  t.StructField('record_id', t.IntegerType()),\n  t.StructField('device_id', t.LongType()),\n  t.StructField('prediction', t.FloatType())\n])\n\n@f.pandas_udf(applyReturnSchema, functionType=f.PandasUDFType.GROUPED_MAP)\ndef apply_model(df_pandas):\n  '''\n  Applies model\n  '''\n  device_id = df_pandas['device_id'].iloc[0]\n  model_path = df_pandas['model_path'].iloc[0]\n  \n  input_columns = ['feature_1', 'feature_2', 'feature_3']\n  X = df_pandas[input_columns]\n  \n  model = mlflow.sklearn.load_model(model_path)\n  prediction = model.predict(X)\n  \n  returnDF = pd.DataFrame({\n    \"record_id\": df_pandas['record_id'],\n    \"prediction\": prediction\n  })\n  returnDF[\"device_id\"] = device_id\n\n  return returnDF\n\npredictionDF = combinedDF.groupby(\"device_id\").apply(apply_model)\ndisplay(predictionDF)", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/html": ["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>record_id</th><th>device_id</th><th>prediction</th></tr></thead><tbody><tr><td>2230</td><td>0</td><td>2.7645466</td></tr><tr><td>2240</td><td>0</td><td>3.4732797</td></tr><tr><td>2250</td><td>0</td><td>1.1655287</td></tr><tr><td>2260</td><td>0</td><td>1.8476894</td></tr><tr><td>2270</td><td>0</td><td>4.6743846</td></tr><tr><td>2280</td><td>0</td><td>2.6057968</td></tr><tr><td>2290</td><td>0</td><td>3.1509695</td></tr><tr><td>2300</td><td>0</td><td>3.2745278</td></tr><tr><td>2310</td><td>0</td><td>3.583009</td></tr><tr><td>2320</td><td>0</td><td>2.7822325</td></tr><tr><td>2330</td><td>0</td><td>3.0175116</td></tr><tr><td>2340</td><td>0</td><td>3.2260532</td></tr><tr><td>2350</td><td>0</td><td>3.5197985</td></tr><tr><td>2360</td><td>0</td><td>3.7866876</td></tr><tr><td>2370</td><td>0</td><td>1.5270185</td></tr><tr><td>2380</td><td>0</td><td>1.8389401</td></tr><tr><td>2390</td><td>0</td><td>5.4011216</td></tr><tr><td>2400</td><td>0</td><td>3.186058</td></tr><tr><td>2410</td><td>0</td><td>5.0828824</td></tr><tr><td>2420</td><td>0</td><td>1.771906</td></tr><tr><td>2430</td><td>0</td><td>5.1816106</td></tr><tr><td>2440</td><td>0</td><td>4.1853933</td></tr><tr><td>2450</td><td>0</td><td>3.4997008</td></tr><tr><td>2460</td><td>0</td><td>2.9162786</td></tr><tr><td>2470</td><td>0</td><td>3.4746532</td></tr><tr><td>2480</td><td>0</td><td>2.849456</td></tr><tr><td>2490</td><td>0</td><td>4.250468</td></tr><tr><td>6670</td><td>0</td><td>3.282454</td></tr><tr><td>6680</td><td>0</td><td>1.4717895</td></tr><tr><td>6690</td><td>0</td><td>2.9016917</td></tr><tr><td>6700</td><td>0</td><td>5.6154237</td></tr><tr><td>6710</td><td>0</td><td>2.012084</td></tr><tr><td>6720</td><td>0</td><td>4.3008976</td></tr><tr><td>6730</td><td>0</td><td>4.6579003</td></tr><tr><td>6740</td><td>0</td><td>3.9577932</td></tr><tr><td>6750</td><td>0</td><td>4.1548514</td></tr><tr><td>6760</td><td>0</td><td>4.369716</td></tr><tr><td>6770</td><td>0</td><td>3.773832</td></tr><tr><td>6780</td><td>0</td><td>2.3882182</td></tr><tr><td>6790</td><td>0</td><td>1.5112449</td></tr><tr><td>6800</td><td>0</td><td>5.0247793</td></tr><tr><td>6810</td><td>0</td><td>1.8621238</td></tr><tr><td>6820</td><td>0</td><td>3.582537</td></tr><tr><td>6830</td><td>0</td><td>2.689874</td></tr><tr><td>6840</td><td>0</td><td>2.948794</td></tr><tr><td>6850</td><td>0</td><td>5.2077975</td></tr><tr><td>6860</td><td>0</td><td>2.903351</td></tr><tr><td>6870</td><td>0</td><td>3.4378107</td></tr><tr><td>6880</td><td>0</td><td>4.3951283</td></tr><tr><td>6890</td><td>0</td><td>1.8056451</td></tr><tr><td>6900</td><td>0</td><td>2.96729</td></tr><tr><td>6910</td><td>0</td><td>4.7540245</td></tr><tr><td>6920</td><td>0</td><td>3.1399415</td></tr><tr><td>6930</td><td>0</td><td>4.65828</td></tr><tr><td>6940</td><td>0</td><td>3.1073532</td></tr><tr><td>840</td><td>0</td><td>3.0459652</td></tr><tr><td>850</td><td>0</td><td>4.1684117</td></tr><tr><td>860</td><td>0</td><td>5.8387647</td></tr><tr><td>870</td><td>0</td><td>3.811041</td></tr><tr><td>880</td><td>0</td><td>4.4334273</td></tr><tr><td>890</td><td>0</td><td>3.3769922</td></tr><tr><td>900</td><td>0</td><td>2.8613095</td></tr><tr><td>910</td><td>0</td><td>2.6314108</td></tr><tr><td>920</td><td>0</td><td>4.322485</td></tr><tr><td>930</td><td>0</td><td>4.841782</td></tr><tr><td>940</td><td>0</td><td>5.308569</td></tr><tr><td>950</td><td>0</td><td>3.6180408</td></tr><tr><td>960</td><td>0</td><td>3.1007078</td></tr><tr><td>970</td><td>0</td><td>5.0193086</td></tr><tr><td>980</td><td>0</td><td>3.46472</td></tr><tr><td>990</td><td>0</td><td>3.5176406</td></tr><tr><td>1000</td><td>0</td><td>3.330081</td></tr><tr><td>1010</td><td>0</td><td>2.6325862</td></tr><tr><td>1020</td><td>0</td><td>2.4969923</td></tr><tr><td>1030</td><td>0</td><td>5.763891</td></tr><tr><td>1040</td><td>0</td><td>4.1533623</td></tr><tr><td>1050</td><td>0</td><td>3.8708355</td></tr><tr><td>1060</td><td>0</td><td>3.009984</td></tr><tr><td>1070</td><td>0</td><td>4.6698065</td></tr><tr><td>1080</td><td>0</td><td>2.5032263</td></tr><tr><td>1090</td><td>0</td><td>3.968521</td></tr><tr><td>1100</td><td>0</td><td>4.268513</td></tr><tr><td>1110</td><td>0</td><td>4.7166176</td></tr><tr><td>3890</td><td>0</td><td>3.3344176</td></tr><tr><td>3900</td><td>0</td><td>2.7728558</td></tr><tr><td>3910</td><td>0</td><td>5.6469097</td></tr><tr><td>3920</td><td>0</td><td>3.2999337</td></tr><tr><td>3930</td><td>0</td><td>4.956028</td></tr><tr><td>3940</td><td>0</td><td>2.9032922</td></tr><tr><td>3950</td><td>0</td><td>1.9021616</td></tr><tr><td>3960</td><td>0</td><td>2.4981875</td></tr><tr><td>3970</td><td>0</td><td>1.970674</td></tr><tr><td>3980</td><td>0</td><td>2.5387626</td></tr><tr><td>3990</td><td>0</td><td>2.6148312</td></tr><tr><td>4000</td><td>0</td><td>2.2283616</td></tr><tr><td>4010</td><td>0</td><td>3.9514155</td></tr><tr><td>4020</td><td>0</td><td>3.4514828</td></tr><tr><td>4030</td><td>0</td><td>3.403113</td></tr><tr><td>4040</td><td>0</td><td>3.9708285</td></tr><tr><td>4050</td><td>0</td><td>4.9401894</td></tr><tr><td>4060</td><td>0</td><td>5.5006456</td></tr><tr><td>4070</td><td>0</td><td>2.2434752</td></tr><tr><td>4080</td><td>0</td><td>2.7451563</td></tr><tr><td>4090</td><td>0</td><td>4.2171006</td></tr><tr><td>4100</td><td>0</td><td>2.8217273</td></tr><tr><td>4110</td><td>0</td><td>2.4949648</td></tr><tr><td>4120</td><td>0</td><td>1.5058714</td></tr><tr><td>4130</td><td>0</td><td>3.4932375</td></tr><tr><td>4140</td><td>0</td><td>2.4027197</td></tr><tr><td>4150</td><td>0</td><td>2.1056166</td></tr><tr><td>4160</td><td>0</td><td>1.7002839</td></tr><tr><td>8340</td><td>0</td><td>1.714514</td></tr><tr><td>8350</td><td>0</td><td>4.920443</td></tr><tr><td>8360</td><td>0</td><td>3.760604</td></tr><tr><td>8370</td><td>0</td><td>3.240015</td></tr><tr><td>8380</td><td>0</td><td>3.2984831</td></tr><tr><td>8390</td><td>0</td><td>2.426301</td></tr><tr><td>8400</td><td>0</td><td>1.919621</td></tr><tr><td>8410</td><td>0</td><td>4.6862173</td></tr><tr><td>8420</td><td>0</td><td>1.8707998</td></tr><tr><td>8430</td><td>0</td><td>3.1371639</td></tr><tr><td>8440</td><td>0</td><td>2.4961498</td></tr><tr><td>8450</td><td>0</td><td>5.3959146</td></tr><tr><td>8460</td><td>0</td><td>3.861724</td></tr><tr><td>8470</td><td>0</td><td>3.4533172</td></tr><tr><td>8480</td><td>0</td><td>1.5503129</td></tr><tr><td>8490</td><td>0</td><td>3.664368</td></tr><tr><td>8500</td><td>0</td><td>2.6598408</td></tr><tr><td>8510</td><td>0</td><td>3.2778308</td></tr><tr><td>8520</td><td>0</td><td>2.7436452</td></tr><tr><td>8530</td><td>0</td><td>1.4108562</td></tr><tr><td>8540</td><td>0</td><td>2.8211458</td></tr><tr><td>8550</td><td>0</td><td>4.2291617</td></tr><tr><td>8560</td><td>0</td><td>3.892825</td></tr><tr><td>8570</td><td>0</td><td>4.6625066</td></tr><tr><td>8580</td><td>0</td><td>3.8446808</td></tr><tr><td>8590</td><td>0</td><td>3.7057245</td></tr><tr><td>8600</td><td>0</td><td>2.7330837</td></tr><tr><td>8610</td><td>0</td><td>4.135774</td></tr><tr><td>5280</td><td>0</td><td>2.750506</td></tr><tr><td>5290</td><td>0</td><td>2.9163532</td></tr><tr><td>5300</td><td>0</td><td>1.5440444</td></tr><tr><td>5310</td><td>0</td><td>4.2409315</td></tr><tr><td>5320</td><td>0</td><td>3.9030716</td></tr><tr><td>5330</td><td>0</td><td>2.4312072</td></tr><tr><td>5340</td><td>0</td><td>5.0885377</td></tr><tr><td>5350</td><td>0</td><td>3.6866362</td></tr><tr><td>5360</td><td>0</td><td>3.0208464</td></tr><tr><td>5370</td><td>0</td><td>4.7733192</td></tr><tr><td>5380</td><td>0</td><td>2.8258507</td></tr><tr><td>5390</td><td>0</td><td>3.3185575</td></tr><tr><td>5400</td><td>0</td><td>1.5355505</td></tr><tr><td>5410</td><td>0</td><td>3.521609</td></tr><tr><td>5420</td><td>0</td><td>1.4025333</td></tr><tr><td>5430</td><td>0</td><td>3.257734</td></tr><tr><td>5440</td><td>0</td><td>2.6701722</td></tr><tr><td>5450</td><td>0</td><td>1.439328</td></tr><tr><td>5460</td><td>0</td><td>0.809115</td></tr><tr><td>5470</td><td>0</td><td>5.520694</td></tr><tr><td>5480</td><td>0</td><td>5.1081343</td></tr><tr><td>5490</td><td>0</td><td>3.5376778</td></tr><tr><td>5500</td><td>0</td><td>4.751895</td></tr><tr><td>5510</td><td>0</td><td>3.173475</td></tr><tr><td>5520</td><td>0</td><td>2.555176</td></tr><tr><td>5530</td><td>0</td><td>2.2054262</td></tr><tr><td>5540</td><td>0</td><td>4.586207</td></tr><tr><td>5550</td><td>0</td><td>3.8698704</td></tr><tr><td>2500</td><td>0</td><td>5.977718</td></tr><tr><td>2510</td><td>0</td><td>5.445575</td></tr><tr><td>2520</td><td>0</td><td>2.2194273</td></tr><tr><td>2530</td><td>0</td><td>2.949692</td></tr><tr><td>2540</td><td>0</td><td>4.0955544</td></tr><tr><td>2550</td><td>0</td><td>1.3970696</td></tr><tr><td>2560</td><td>0</td><td>2.1445642</td></tr><tr><td>2570</td><td>0</td><td>3.4936538</td></tr><tr><td>2580</td><td>0</td><td>4.3889685</td></tr><tr><td>2590</td><td>0</td><td>4.0782995</td></tr><tr><td>2600</td><td>0</td><td>2.8473413</td></tr><tr><td>2610</td><td>0</td><td>5.874624</td></tr><tr><td>2620</td><td>0</td><td>1.0415455</td></tr><tr><td>2630</td><td>0</td><td>3.061826</td></tr><tr><td>2640</td><td>0</td><td>3.3327637</td></tr><tr><td>2650</td><td>0</td><td>5.3682556</td></tr><tr><td>2660</td><td>0</td><td>3.273944</td></tr><tr><td>2670</td><td>0</td><td>4.226705</td></tr><tr><td>2680</td><td>0</td><td>3.0566187</td></tr><tr><td>2690</td><td>0</td><td>4.2498045</td></tr><tr><td>2700</td><td>0</td><td>2.4775417</td></tr><tr><td>2710</td><td>0</td><td>2.4491525</td></tr><tr><td>2720</td><td>0</td><td>4.6496916</td></tr><tr><td>2730</td><td>0</td><td>3.0321836</td></tr><tr><td>2740</td><td>0</td><td>2.3096464</td></tr><tr><td>2750</td><td>0</td><td>4.0656333</td></tr><tr><td>2760</td><td>0</td><td>2.5690312</td></tr><tr><td>2770</td><td>0</td><td>4.444375</td></tr><tr><td>1670</td><td>0</td><td>4.2924495</td></tr><tr><td>1680</td><td>0</td><td>2.8758395</td></tr><tr><td>1690</td><td>0</td><td>2.147374</td></tr><tr><td>1700</td><td>0</td><td>3.5700932</td></tr><tr><td>1710</td><td>0</td><td>5.2173653</td></tr><tr><td>1720</td><td>0</td><td>4.4300733</td></tr><tr><td>1730</td><td>0</td><td>5.144773</td></tr><tr><td>1740</td><td>0</td><td>3.3742595</td></tr><tr><td>1750</td><td>0</td><td>4.701446</td></tr><tr><td>1760</td><td>0</td><td>1.3088663</td></tr><tr><td>1770</td><td>0</td><td>4.2559285</td></tr><tr><td>1780</td><td>0</td><td>4.0565295</td></tr><tr><td>1790</td><td>0</td><td>1.98264</td></tr><tr><td>1800</td><td>0</td><td>3.6254961</td></tr><tr><td>1810</td><td>0</td><td>2.4469662</td></tr><tr><td>1820</td><td>0</td><td>5.8548594</td></tr><tr><td>1830</td><td>0</td><td>3.00039</td></tr><tr><td>1840</td><td>0</td><td>2.978898</td></tr><tr><td>1850</td><td>0</td><td>4.749421</td></tr><tr><td>1860</td><td>0</td><td>3.8109052</td></tr><tr><td>1870</td><td>0</td><td>5.1510286</td></tr><tr><td>1880</td><td>0</td><td>2.4291997</td></tr><tr><td>1890</td><td>0</td><td>2.973147</td></tr><tr><td>1900</td><td>0</td><td>2.5010443</td></tr><tr><td>1910</td><td>0</td><td>4.0007133</td></tr><tr><td>1920</td><td>0</td><td>4.6649666</td></tr><tr><td>1930</td><td>0</td><td>2.2563465</td></tr><tr><td>1940</td><td>0</td><td>2.9761388</td></tr><tr><td>4170</td><td>0</td><td>2.9759767</td></tr><tr><td>4180</td><td>0</td><td>3.3915784</td></tr><tr><td>4190</td><td>0</td><td>4.5982857</td></tr><tr><td>4200</td><td>0</td><td>4.187102</td></tr><tr><td>4210</td><td>0</td><td>2.7593403</td></tr><tr><td>4220</td><td>0</td><td>2.5032067</td></tr><tr><td>4230</td><td>0</td><td>5.1907845</td></tr><tr><td>4240</td><td>0</td><td>2.6440606</td></tr><tr><td>4250</td><td>0</td><td>4.7529826</td></tr><tr><td>4260</td><td>0</td><td>3.0071936</td></tr><tr><td>4270</td><td>0</td><td>4.1254854</td></tr><tr><td>4280</td><td>0</td><td>2.243255</td></tr><tr><td>4290</td><td>0</td><td>2.6502752</td></tr><tr><td>4300</td><td>0</td><td>2.7579384</td></tr><tr><td>4310</td><td>0</td><td>2.5918267</td></tr><tr><td>4320</td><td>0</td><td>3.8538907</td></tr><tr><td>4330</td><td>0</td><td>4.169538</td></tr><tr><td>4340</td><td>0</td><td>4.0009</td></tr><tr><td>4350</td><td>0</td><td>3.9605145</td></tr><tr><td>4360</td><td>0</td><td>2.772806</td></tr><tr><td>4370</td><td>0</td><td>3.7775347</td></tr><tr><td>4380</td><td>0</td><td>4.704485</td></tr><tr><td>4390</td><td>0</td><td>0.93256354</td></tr><tr><td>4400</td><td>0</td><td>3.170452</td></tr><tr><td>4410</td><td>0</td><td>3.2398322</td></tr><tr><td>4420</td><td>0</td><td>3.8723075</td></tr><tr><td>4430</td><td>0</td><td>4.1363716</td></tr><tr><td>4440</td><td>0</td><td>2.4032907</td></tr><tr><td>9730</td><td>0</td><td>4.418973</td></tr><tr><td>9740</td><td>0</td><td>2.7090707</td></tr><tr><td>9750</td><td>0</td><td>4.7769365</td></tr><tr><td>9760</td><td>0</td><td>4.003559</td></tr><tr><td>9770</td><td>0</td><td>4.5107217</td></tr><tr><td>9780</td><td>0</td><td>3.3850393</td></tr><tr><td>9790</td><td>0</td><td>1.560333</td></tr><tr><td>9800</td><td>0</td><td>5.278772</td></tr><tr><td>9810</td><td>0</td><td>2.9762607</td></tr><tr><td>9820</td><td>0</td><td>3.0749693</td></tr><tr><td>9830</td><td>0</td><td>4.1489987</td></tr><tr><td>9840</td><td>0</td><td>2.5902007</td></tr><tr><td>9850</td><td>0</td><td>5.1124926</td></tr><tr><td>9860</td><td>0</td><td>2.3624613</td></tr><tr><td>9870</td><td>0</td><td>2.3666897</td></tr><tr><td>9880</td><td>0</td><td>1.8534058</td></tr><tr><td>9890</td><td>0</td><td>2.3233135</td></tr><tr><td>9900</td><td>0</td><td>2.9035027</td></tr><tr><td>9910</td><td>0</td><td>3.592664</td></tr><tr><td>9920</td><td>0</td><td>1.9960719</td></tr><tr><td>9930</td><td>0</td><td>4.736552</td></tr><tr><td>9940</td><td>0</td><td>2.20475</td></tr><tr><td>9950</td><td>0</td><td>3.9676213</td></tr><tr><td>9960</td><td>0</td><td>2.628196</td></tr><tr><td>9970</td><td>0</td><td>5.048287</td></tr><tr><td>9980</td><td>0</td><td>3.0371692</td></tr><tr><td>9990</td><td>0</td><td>4.9956503</td></tr><tr><td>6120</td><td>0</td><td>3.783331</td></tr><tr><td>6130</td><td>0</td><td>3.341505</td></tr><tr><td>6140</td><td>0</td><td>1.5486174</td></tr><tr><td>6150</td><td>0</td><td>5.914674</td></tr><tr><td>6160</td><td>0</td><td>2.9836757</td></tr><tr><td>6170</td><td>0</td><td>1.6757451</td></tr><tr><td>6180</td><td>0</td><td>3.4023986</td></tr><tr><td>6190</td><td>0</td><td>4.483131</td></tr><tr><td>6200</td><td>0</td><td>4.0516343</td></tr><tr><td>6210</td><td>0</td><td>4.326112</td></tr><tr><td>6220</td><td>0</td><td>5.3510785</td></tr><tr><td>6230</td><td>0</td><td>4.481614</td></tr><tr><td>6240</td><td>0</td><td>2.6757889</td></tr><tr><td>6250</td><td>0</td><td>4.2620792</td></tr><tr><td>6260</td><td>0</td><td>1.5733205</td></tr><tr><td>6270</td><td>0</td><td>4.7954516</td></tr><tr><td>6280</td><td>0</td><td>4.0234523</td></tr><tr><td>6290</td><td>0</td><td>3.4611804</td></tr><tr><td>6300</td><td>0</td><td>3.2690704</td></tr><tr><td>6310</td><td>0</td><td>3.9995868</td></tr><tr><td>6320</td><td>0</td><td>1.9104214</td></tr><tr><td>6330</td><td>0</td><td>2.1557193</td></tr><tr><td>6340</td><td>0</td><td>5.6123586</td></tr><tr><td>6350</td><td>0</td><td>3.5105896</td></tr><tr><td>6360</td><td>0</td><td>4.04016</td></tr><tr><td>6370</td><td>0</td><td>3.2614615</td></tr><tr><td>6380</td><td>0</td><td>5.4808435</td></tr><tr><td>280</td><td>0</td><td>6.1036367</td></tr><tr><td>290</td><td>0</td><td>1.9570568</td></tr><tr><td>300</td><td>0</td><td>3.2506301</td></tr><tr><td>310</td><td>0</td><td>1.9580792</td></tr><tr><td>320</td><td>0</td><td>4.8047624</td></tr><tr><td>330</td><td>0</td><td>3.8987486</td></tr><tr><td>340</td><td>0</td><td>3.6178455</td></tr><tr><td>350</td><td>0</td><td>5.069439</td></tr><tr><td>360</td><td>0</td><td>4.43988</td></tr><tr><td>370</td><td>0</td><td>1.8088479</td></tr><tr><td>380</td><td>0</td><td>3.743536</td></tr><tr><td>390</td><td>0</td><td>2.163684</td></tr><tr><td>400</td><td>0</td><td>4.2556615</td></tr><tr><td>410</td><td>0</td><td>3.6782718</td></tr><tr><td>420</td><td>0</td><td>2.9961538</td></tr><tr><td>430</td><td>0</td><td>4.609613</td></tr><tr><td>440</td><td>0</td><td>2.6008916</td></tr><tr><td>450</td><td>0</td><td>4.101966</td></tr><tr><td>460</td><td>0</td><td>2.9799268</td></tr><tr><td>470</td><td>0</td><td>2.9439485</td></tr><tr><td>480</td><td>0</td><td>3.9720683</td></tr><tr><td>490</td><td>0</td><td>4.651134</td></tr><tr><td>500</td><td>0</td><td>5.3280053</td></tr><tr><td>510</td><td>0</td><td>5.29929</td></tr><tr><td>520</td><td>0</td><td>3.7944117</td></tr><tr><td>530</td><td>0</td><td>2.4353838</td></tr><tr><td>540</td><td>0</td><td>2.7663636</td></tr><tr><td>550</td><td>0</td><td>3.4458234</td></tr><tr><td>6950</td><td>0</td><td>2.921849</td></tr><tr><td>6960</td><td>0</td><td>3.1364365</td></tr><tr><td>6970</td><td>0</td><td>3.2349966</td></tr><tr><td>6980</td><td>0</td><td>5.259253</td></tr><tr><td>6990</td><td>0</td><td>2.834259</td></tr><tr><td>7000</td><td>0</td><td>3.4616888</td></tr><tr><td>7010</td><td>0</td><td>3.5052447</td></tr><tr><td>7020</td><td>0</td><td>5.7741227</td></tr><tr><td>7030</td><td>0</td><td>4.2104154</td></tr><tr><td>7040</td><td>0</td><td>2.4317527</td></tr><tr><td>7050</td><td>0</td><td>2.4457114</td></tr><tr><td>7060</td><td>0</td><td>4.7290025</td></tr><tr><td>7070</td><td>0</td><td>3.842623</td></tr><tr><td>7080</td><td>0</td><td>3.6149313</td></tr><tr><td>7090</td><td>0</td><td>3.4650483</td></tr><tr><td>7100</td><td>0</td><td>6.0243263</td></tr><tr><td>7110</td><td>0</td><td>1.6949631</td></tr><tr><td>7120</td><td>0</td><td>2.521436</td></tr><tr><td>7130</td><td>0</td><td>3.807384</td></tr><tr><td>7140</td><td>0</td><td>5.803836</td></tr><tr><td>7150</td><td>0</td><td>2.3278427</td></tr><tr><td>7160</td><td>0</td><td>5.28706</td></tr><tr><td>7170</td><td>0</td><td>3.3439658</td></tr><tr><td>7180</td><td>0</td><td>3.2098422</td></tr><tr><td>7190</td><td>0</td><td>4.522699</td></tr><tr><td>7200</td><td>0</td><td>3.045876</td></tr><tr><td>7210</td><td>0</td><td>3.3400416</td></tr><tr><td>7220</td><td>0</td><td>4.477431</td></tr><tr><td>8620</td><td>0</td><td>2.1790566</td></tr><tr><td>8630</td><td>0</td><td>2.38232</td></tr><tr><td>8640</td><td>0</td><td>2.6007113</td></tr><tr><td>8650</td><td>0</td><td>4.8601947</td></tr><tr><td>8660</td><td>0</td><td>3.0784736</td></tr><tr><td>8670</td><td>0</td><td>2.4453964</td></tr><tr><td>8680</td><td>0</td><td>2.552148</td></tr><tr><td>8690</td><td>0</td><td>4.382736</td></tr><tr><td>8700</td><td>0</td><td>3.5405943</td></tr><tr><td>8710</td><td>0</td><td>2.4542043</td></tr><tr><td>8720</td><td>0</td><td>2.6509025</td></tr><tr><td>8730</td><td>0</td><td>3.8334978</td></tr><tr><td>8740</td><td>0</td><td>2.6128726</td></tr><tr><td>8750</td><td>0</td><td>1.4143597</td></tr><tr><td>8760</td><td>0</td><td>3.6156976</td></tr><tr><td>8770</td><td>0</td><td>0.9312776</td></tr><tr><td>8780</td><td>0</td><td>3.309487</td></tr><tr><td>8790</td><td>0</td><td>3.497609</td></tr><tr><td>8800</td><td>0</td><td>4.622747</td></tr><tr><td>8810</td><td>0</td><td>1.1897188</td></tr><tr><td>8820</td><td>0</td><td>3.608955</td></tr><tr><td>8830</td><td>0</td><td>4.2653365</td></tr><tr><td>8840</td><td>0</td><td>4.070457</td></tr><tr><td>8850</td><td>0</td><td>3.3258169</td></tr><tr><td>8860</td><td>0</td><td>3.3288155</td></tr><tr><td>8870</td><td>0</td><td>2.6166804</td></tr><tr><td>8880</td><td>0</td><td>2.4976246</td></tr><tr><td>2780</td><td>0</td><td>4.0906177</td></tr><tr><td>2790</td><td>0</td><td>3.1308403</td></tr><tr><td>2800</td><td>0</td><td>5.123084</td></tr><tr><td>2810</td><td>0</td><td>3.5633852</td></tr><tr><td>2820</td><td>0</td><td>3.7989895</td></tr><tr><td>2830</td><td>0</td><td>4.3370776</td></tr><tr><td>2840</td><td>0</td><td>5.290895</td></tr><tr><td>2850</td><td>0</td><td>2.6786044</td></tr><tr><td>2860</td><td>0</td><td>3.515446</td></tr><tr><td>2870</td><td>0</td><td>3.8343296</td></tr><tr><td>2880</td><td>0</td><td>4.7998247</td></tr><tr><td>2890</td><td>0</td><td>1.7992905</td></tr><tr><td>2900</td><td>0</td><td>2.1825106</td></tr><tr><td>2910</td><td>0</td><td>4.401441</td></tr><tr><td>2920</td><td>0</td><td>3.0896542</td></tr><tr><td>2930</td><td>0</td><td>2.6433027</td></tr><tr><td>2940</td><td>0</td><td>2.9575763</td></tr><tr><td>2950</td><td>0</td><td>3.8440108</td></tr><tr><td>2960</td><td>0</td><td>3.5593767</td></tr><tr><td>2970</td><td>0</td><td>3.0641506</td></tr><tr><td>2980</td><td>0</td><td>3.8278146</td></tr><tr><td>2990</td><td>0</td><td>3.5747411</td></tr><tr><td>3000</td><td>0</td><td>4.773361</td></tr><tr><td>3010</td><td>0</td><td>4.950454</td></tr><tr><td>3020</td><td>0</td><td>3.4332845</td></tr><tr><td>3030</td><td>0</td><td>3.6557982</td></tr><tr><td>3040</td><td>0</td><td>1.9049251</td></tr><tr><td>3050</td><td>0</td><td>2.327955</td></tr><tr><td>3340</td><td>0</td><td>3.6184766</td></tr><tr><td>3350</td><td>0</td><td>2.8396528</td></tr><tr><td>3360</td><td>0</td><td>2.9373395</td></tr><tr><td>3370</td><td>0</td><td>3.3329394</td></tr><tr><td>3380</td><td>0</td><td>4.5516896</td></tr><tr><td>3390</td><td>0</td><td>3.9630013</td></tr><tr><td>3400</td><td>0</td><td>2.4922385</td></tr><tr><td>3410</td><td>0</td><td>3.3761215</td></tr><tr><td>3420</td><td>0</td><td>1.4833345</td></tr><tr><td>3430</td><td>0</td><td>2.7143347</td></tr><tr><td>3440</td><td>0</td><td>4.0697584</td></tr><tr><td>3450</td><td>0</td><td>3.8833814</td></tr><tr><td>3460</td><td>0</td><td>3.6349087</td></tr><tr><td>3470</td><td>0</td><td>2.4527614</td></tr><tr><td>3480</td><td>0</td><td>4.4561753</td></tr><tr><td>3490</td><td>0</td><td>3.6453426</td></tr><tr><td>3500</td><td>0</td><td>4.784964</td></tr><tr><td>3510</td><td>0</td><td>2.674591</td></tr><tr><td>3520</td><td>0</td><td>4.866339</td></tr><tr><td>3530</td><td>0</td><td>2.9965432</td></tr><tr><td>3540</td><td>0</td><td>5.845656</td></tr><tr><td>3550</td><td>0</td><td>3.8214028</td></tr><tr><td>3560</td><td>0</td><td>4.702499</td></tr><tr><td>3570</td><td>0</td><td>4.2014318</td></tr><tr><td>3580</td><td>0</td><td>3.416518</td></tr><tr><td>3590</td><td>0</td><td>4.164343</td></tr><tr><td>3600</td><td>0</td><td>2.9853957</td></tr><tr><td>3610</td><td>0</td><td>2.3445127</td></tr><tr><td>7230</td><td>0</td><td>3.0707362</td></tr><tr><td>7240</td><td>0</td><td>2.254767</td></tr><tr><td>7250</td><td>0</td><td>5.668529</td></tr><tr><td>7260</td><td>0</td><td>3.592203</td></tr><tr><td>7270</td><td>0</td><td>4.2360735</td></tr><tr><td>7280</td><td>0</td><td>3.634245</td></tr><tr><td>7290</td><td>0</td><td>3.394344</td></tr><tr><td>7300</td><td>0</td><td>1.4022999</td></tr><tr><td>7310</td><td>0</td><td>5.809558</td></tr><tr><td>7320</td><td>0</td><td>4.6843452</td></tr><tr><td>7330</td><td>0</td><td>3.073839</td></tr><tr><td>7340</td><td>0</td><td>3.853301</td></tr><tr><td>7350</td><td>0</td><td>3.1355052</td></tr><tr><td>7360</td><td>0</td><td>3.6104238</td></tr><tr><td>7370</td><td>0</td><td>2.873504</td></tr><tr><td>7380</td><td>0</td><td>3.5441067</td></tr><tr><td>7390</td><td>0</td><td>3.1478379</td></tr><tr><td>7400</td><td>0</td><td>3.094712</td></tr><tr><td>7410</td><td>0</td><td>3.7089074</td></tr><tr><td>7420</td><td>0</td><td>3.7959342</td></tr><tr><td>7430</td><td>0</td><td>3.8073506</td></tr><tr><td>7440</td><td>0</td><td>4.0884585</td></tr><tr><td>7450</td><td>0</td><td>2.3548145</td></tr><tr><td>7460</td><td>0</td><td>2.5300221</td></tr><tr><td>7470</td><td>0</td><td>4.4822197</td></tr><tr><td>7480</td><td>0</td><td>4.660115</td></tr><tr><td>7490</td><td>0</td><td>4.870851</td></tr><tr><td>7780</td><td>0</td><td>2.9205606</td></tr><tr><td>7790</td><td>0</td><td>3.488345</td></tr><tr><td>7800</td><td>0</td><td>4.324112</td></tr><tr><td>7810</td><td>0</td><td>2.689693</td></tr><tr><td>7820</td><td>0</td><td>3.2635183</td></tr><tr><td>7830</td><td>0</td><td>1.4815937</td></tr><tr><td>7840</td><td>0</td><td>4.040241</td></tr><tr><td>7850</td><td>0</td><td>3.6596246</td></tr><tr><td>7860</td><td>0</td><td>2.557063</td></tr><tr><td>7870</td><td>0</td><td>3.7842674</td></tr><tr><td>7880</td><td>0</td><td>4.425108</td></tr><tr><td>7890</td><td>0</td><td>3.2217393</td></tr><tr><td>7900</td><td>0</td><td>2.2833986</td></tr><tr><td>7910</td><td>0</td><td>3.7197645</td></tr><tr><td>7920</td><td>0</td><td>4.6866045</td></tr><tr><td>7930</td><td>0</td><td>5.3710933</td></tr><tr><td>7940</td><td>0</td><td>0.7629236</td></tr><tr><td>7950</td><td>0</td><td>3.3349748</td></tr><tr><td>7960</td><td>0</td><td>1.2235054</td></tr><tr><td>7970</td><td>0</td><td>3.9025836</td></tr><tr><td>7980</td><td>0</td><td>4.089507</td></tr><tr><td>7990</td><td>0</td><td>3.0520873</td></tr><tr><td>8000</td><td>0</td><td>3.801017</td></tr><tr><td>8010</td><td>0</td><td>5.057764</td></tr><tr><td>8020</td><td>0</td><td>2.3992026</td></tr><tr><td>8030</td><td>0</td><td>3.5451427</td></tr><tr><td>8040</td><td>0</td><td>3.6484246</td></tr><tr><td>8050</td><td>0</td><td>4.0441513</td></tr><tr><td>3620</td><td>0</td><td>2.7461994</td></tr><tr><td>3630</td><td>0</td><td>3.0062308</td></tr><tr><td>3640</td><td>0</td><td>4.576435</td></tr><tr><td>3650</td><td>0</td><td>2.5954838</td></tr><tr><td>3660</td><td>0</td><td>3.9144013</td></tr><tr><td>3670</td><td>0</td><td>4.615717</td></tr><tr><td>3680</td><td>0</td><td>4.3688097</td></tr><tr><td>3690</td><td>0</td><td>3.0551813</td></tr><tr><td>3700</td><td>0</td><td>3.3207018</td></tr><tr><td>3710</td><td>0</td><td>3.1750607</td></tr><tr><td>3720</td><td>0</td><td>1.4357885</td></tr><tr><td>3730</td><td>0</td><td>3.0635157</td></tr><tr><td>3740</td><td>0</td><td>5.2406983</td></tr><tr><td>3750</td><td>0</td><td>2.5857706</td></tr><tr><td>3760</td><td>0</td><td>4.019735</td></tr><tr><td>3770</td><td>0</td><td>3.1188684</td></tr><tr><td>3780</td><td>0</td><td>1.7181323</td></tr><tr><td>3790</td><td>0</td><td>4.7407985</td></tr><tr><td>3800</td><td>0</td><td>3.0635185</td></tr><tr><td>3810</td><td>0</td><td>2.555369</td></tr><tr><td>3820</td><td>0</td><td>4.488572</td></tr><tr><td>3830</td><td>0</td><td>2.4954238</td></tr><tr><td>3840</td><td>0</td><td>2.3001556</td></tr><tr><td>3850</td><td>0</td><td>5.2985835</td></tr><tr><td>3860</td><td>0</td><td>2.3165698</td></tr><tr><td>3870</td><td>0</td><td>3.3349528</td></tr><tr><td>3880</td><td>0</td><td>3.9797153</td></tr><tr><td>4730</td><td>0</td><td>3.650646</td></tr><tr><td>4740</td><td>0</td><td>4.8788013</td></tr><tr><td>4750</td><td>0</td><td>5.3448243</td></tr><tr><td>4760</td><td>0</td><td>2.6048214</td></tr><tr><td>4770</td><td>0</td><td>2.2460077</td></tr><tr><td>4780</td><td>0</td><td>5.041616</td></tr><tr><td>4790</td><td>0</td><td>3.2260602</td></tr><tr><td>4800</td><td>0</td><td>3.5474708</td></tr><tr><td>4810</td><td>0</td><td>4.228092</td></tr><tr><td>4820</td><td>0</td><td>5.6793637</td></tr><tr><td>4830</td><td>0</td><td>3.5699177</td></tr><tr><td>4840</td><td>0</td><td>1.3794619</td></tr><tr><td>4850</td><td>0</td><td>4.4366703</td></tr><tr><td>4860</td><td>0</td><td>3.4256015</td></tr><tr><td>4870</td><td>0</td><td>4.3453555</td></tr><tr><td>4880</td><td>0</td><td>2.3278186</td></tr><tr><td>4890</td><td>0</td><td>2.5268548</td></tr><tr><td>4900</td><td>0</td><td>4.266855</td></tr><tr><td>4910</td><td>0</td><td>5.7659297</td></tr><tr><td>4920</td><td>0</td><td>2.451697</td></tr><tr><td>4930</td><td>0</td><td>2.320594</td></tr><tr><td>4940</td><td>0</td><td>4.55766</td></tr><tr><td>4950</td><td>0</td><td>2.405081</td></tr><tr><td>4960</td><td>0</td><td>5.0737348</td></tr><tr><td>4970</td><td>0</td><td>1.4949989</td></tr><tr><td>4980</td><td>0</td><td>3.141896</td></tr><tr><td>4990</td><td>0</td><td>3.0224187</td></tr><tr><td>8060</td><td>0</td><td>4.009827</td></tr><tr><td>8070</td><td>0</td><td>4.027769</td></tr><tr><td>8080</td><td>0</td><td>3.7425306</td></tr><tr><td>8090</td><td>0</td><td>4.4412246</td></tr><tr><td>8100</td><td>0</td><td>2.8137677</td></tr><tr><td>8110</td><td>0</td><td>4.223885</td></tr><tr><td>8120</td><td>0</td><td>3.32363</td></tr><tr><td>8130</td><td>0</td><td>3.114589</td></tr><tr><td>8140</td><td>0</td><td>2.010016</td></tr><tr><td>8150</td><td>0</td><td>2.5102007</td></tr><tr><td>8160</td><td>0</td><td>1.6939051</td></tr><tr><td>8170</td><td>0</td><td>4.7133656</td></tr><tr><td>8180</td><td>0</td><td>4.317361</td></tr><tr><td>8190</td><td>0</td><td>4.573805</td></tr><tr><td>8200</td><td>0</td><td>5.5420084</td></tr><tr><td>8210</td><td>0</td><td>4.4830394</td></tr><tr><td>8220</td><td>0</td><td>3.4501607</td></tr><tr><td>8230</td><td>0</td><td>5.250882</td></tr><tr><td>8240</td><td>0</td><td>4.6485424</td></tr><tr><td>8250</td><td>0</td><td>3.5409946</td></tr><tr><td>8260</td><td>0</td><td>2.8255854</td></tr><tr><td>8270</td><td>0</td><td>1.5557424</td></tr><tr><td>8280</td><td>0</td><td>4.7568736</td></tr><tr><td>8290</td><td>0</td><td>3.7507186</td></tr><tr><td>8300</td><td>0</td><td>6.1895714</td></tr><tr><td>8310</td><td>0</td><td>4.9506574</td></tr><tr><td>8320</td><td>0</td><td>3.511091</td></tr><tr><td>8330</td><td>0</td><td>2.1623979</td></tr><tr><td>9170</td><td>0</td><td>4.532165</td></tr><tr><td>9180</td><td>0</td><td>3.8199823</td></tr><tr><td>9190</td><td>0</td><td>5.1087203</td></tr><tr><td>9200</td><td>0</td><td>3.5769274</td></tr><tr><td>9210</td><td>0</td><td>4.1617002</td></tr><tr><td>9220</td><td>0</td><td>2.6860964</td></tr><tr><td>9230</td><td>0</td><td>4.4059787</td></tr><tr><td>9240</td><td>0</td><td>3.3563454</td></tr><tr><td>9250</td><td>0</td><td>4.331228</td></tr><tr><td>9260</td><td>0</td><td>3.395431</td></tr><tr><td>9270</td><td>0</td><td>4.4229083</td></tr><tr><td>9280</td><td>0</td><td>2.229378</td></tr><tr><td>9290</td><td>0</td><td>3.8824592</td></tr><tr><td>9300</td><td>0</td><td>2.7165186</td></tr><tr><td>9310</td><td>0</td><td>5.188903</td></tr><tr><td>9320</td><td>0</td><td>4.787778</td></tr><tr><td>9330</td><td>0</td><td>4.5678406</td></tr><tr><td>9340</td><td>0</td><td>4.9610066</td></tr><tr><td>9350</td><td>0</td><td>4.329303</td></tr><tr><td>9360</td><td>0</td><td>1.9079018</td></tr><tr><td>9370</td><td>0</td><td>3.5762012</td></tr><tr><td>9380</td><td>0</td><td>3.0451982</td></tr><tr><td>9390</td><td>0</td><td>3.092588</td></tr><tr><td>9400</td><td>0</td><td>2.0686245</td></tr><tr><td>9410</td><td>0</td><td>3.8542092</td></tr><tr><td>9420</td><td>0</td><td>5.0815597</td></tr><tr><td>9430</td><td>0</td><td>4.646204</td></tr><tr><td>9440</td><td>0</td><td>2.773665</td></tr><tr><td>1950</td><td>0</td><td>3.2810848</td></tr><tr><td>1960</td><td>0</td><td>2.005975</td></tr><tr><td>1970</td><td>0</td><td>3.744895</td></tr><tr><td>1980</td><td>0</td><td>4.25458</td></tr><tr><td>1990</td><td>0</td><td>2.624044</td></tr><tr><td>2000</td><td>0</td><td>4.5326104</td></tr><tr><td>2010</td><td>0</td><td>2.50302</td></tr><tr><td>2020</td><td>0</td><td>3.839315</td></tr><tr><td>2030</td><td>0</td><td>1.4184407</td></tr><tr><td>2040</td><td>0</td><td>4.8082633</td></tr><tr><td>2050</td><td>0</td><td>2.6463354</td></tr><tr><td>2060</td><td>0</td><td>3.2914367</td></tr><tr><td>2070</td><td>0</td><td>4.1516504</td></tr><tr><td>2080</td><td>0</td><td>2.996601</td></tr><tr><td>2090</td><td>0</td><td>2.0956855</td></tr><tr><td>2100</td><td>0</td><td>6.3403482</td></tr><tr><td>2110</td><td>0</td><td>2.670322</td></tr><tr><td>2120</td><td>0</td><td>4.894004</td></tr><tr><td>2130</td><td>0</td><td>2.9282646</td></tr><tr><td>2140</td><td>0</td><td>4.170543</td></tr><tr><td>2150</td><td>0</td><td>1.8672489</td></tr><tr><td>2160</td><td>0</td><td>5.130225</td></tr><tr><td>2170</td><td>0</td><td>4.556874</td></tr><tr><td>2180</td><td>0</td><td>4.613033</td></tr><tr><td>2190</td><td>0</td><td>3.0587277</td></tr><tr><td>2200</td><td>0</td><td>2.2812097</td></tr><tr><td>2210</td><td>0</td><td>5.1039276</td></tr><tr><td>2220</td><td>0</td><td>3.9914224</td></tr><tr><td>0</td><td>0</td><td>4.3238645</td></tr><tr><td>10</td><td>0</td><td>4.3341236</td></tr><tr><td>20</td><td>0</td><td>3.9186695</td></tr><tr><td>30</td><td>0</td><td>1.9134641</td></tr><tr><td>40</td><td>0</td><td>4.646849</td></tr><tr><td>50</td><td>0</td><td>2.9796808</td></tr><tr><td>60</td><td>0</td><td>3.454329</td></tr><tr><td>70</td><td>0</td><td>2.8765893</td></tr><tr><td>80</td><td>0</td><td>3.0718145</td></tr><tr><td>90</td><td>0</td><td>3.500354</td></tr><tr><td>100</td><td>0</td><td>3.6358082</td></tr><tr><td>110</td><td>0</td><td>1.5472001</td></tr><tr><td>120</td><td>0</td><td>1.3804601</td></tr><tr><td>130</td><td>0</td><td>3.3355992</td></tr><tr><td>140</td><td>0</td><td>1.4332169</td></tr><tr><td>150</td><td>0</td><td>3.8080153</td></tr><tr><td>160</td><td>0</td><td>4.058708</td></tr><tr><td>170</td><td>0</td><td>2.5694458</td></tr><tr><td>180</td><td>0</td><td>1.4674668</td></tr><tr><td>190</td><td>0</td><td>1.8214922</td></tr><tr><td>200</td><td>0</td><td>3.7067947</td></tr><tr><td>210</td><td>0</td><td>3.1975565</td></tr><tr><td>220</td><td>0</td><td>3.1736484</td></tr><tr><td>230</td><td>0</td><td>3.666431</td></tr><tr><td>240</td><td>0</td><td>3.0604248</td></tr><tr><td>250</td><td>0</td><td>2.513164</td></tr><tr><td>260</td><td>0</td><td>2.293021</td></tr><tr><td>270</td><td>0</td><td>2.6568723</td></tr><tr><td>3060</td><td>0</td><td>2.5085852</td></tr><tr><td>3070</td><td>0</td><td>1.5400708</td></tr><tr><td>3080</td><td>0</td><td>3.039228</td></tr><tr><td>3090</td><td>0</td><td>2.2811468</td></tr><tr><td>3100</td><td>0</td><td>4.942261</td></tr><tr><td>3110</td><td>0</td><td>2.7700076</td></tr><tr><td>3120</td><td>0</td><td>4.4738135</td></tr><tr><td>3130</td><td>0</td><td>4.9238877</td></tr><tr><td>3140</td><td>0</td><td>2.1331336</td></tr><tr><td>3150</td><td>0</td><td>3.9894342</td></tr><tr><td>3160</td><td>0</td><td>2.9653895</td></tr><tr><td>3170</td><td>0</td><td>3.20864</td></tr><tr><td>3180</td><td>0</td><td>2.7758512</td></tr><tr><td>3190</td><td>0</td><td>3.6779783</td></tr><tr><td>3200</td><td>0</td><td>4.3107014</td></tr><tr><td>3210</td><td>0</td><td>3.9922068</td></tr><tr><td>3220</td><td>0</td><td>5.020353</td></tr><tr><td>3230</td><td>0</td><td>5.4013896</td></tr><tr><td>3240</td><td>0</td><td>4.5011683</td></tr><tr><td>3250</td><td>0</td><td>4.558624</td></tr><tr><td>3260</td><td>0</td><td>2.6824644</td></tr><tr><td>3270</td><td>0</td><td>2.899442</td></tr><tr><td>3280</td><td>0</td><td>2.1453178</td></tr><tr><td>3290</td><td>0</td><td>3.8297102</td></tr><tr><td>3300</td><td>0</td><td>3.5907629</td></tr><tr><td>3310</td><td>0</td><td>2.8595243</td></tr><tr><td>3320</td><td>0</td><td>2.658576</td></tr><tr><td>3330</td><td>0</td><td>2.1947908</td></tr><tr><td>4450</td><td>0</td><td>1.807959</td></tr><tr><td>4460</td><td>0</td><td>3.2108123</td></tr><tr><td>4470</td><td>0</td><td>3.453944</td></tr><tr><td>4480</td><td>0</td><td>4.141134</td></tr><tr><td>4490</td><td>0</td><td>2.1410148</td></tr><tr><td>4500</td><td>0</td><td>3.5458028</td></tr><tr><td>4510</td><td>0</td><td>3.4607198</td></tr><tr><td>4520</td><td>0</td><td>4.342322</td></tr><tr><td>4530</td><td>0</td><td>4.130441</td></tr><tr><td>4540</td><td>0</td><td>4.3100324</td></tr><tr><td>4550</td><td>0</td><td>4.3486423</td></tr><tr><td>4560</td><td>0</td><td>3.6002295</td></tr><tr><td>4570</td><td>0</td><td>3.0457795</td></tr><tr><td>4580</td><td>0</td><td>3.4066172</td></tr><tr><td>4590</td><td>0</td><td>2.1856194</td></tr><tr><td>4600</td><td>0</td><td>2.8543243</td></tr><tr><td>4610</td><td>0</td><td>2.0273983</td></tr><tr><td>4620</td><td>0</td><td>5.3036785</td></tr><tr><td>4630</td><td>0</td><td>4.2127376</td></tr><tr><td>4640</td><td>0</td><td>4.951241</td></tr><tr><td>4650</td><td>0</td><td>4.74842</td></tr><tr><td>4660</td><td>0</td><td>3.9693527</td></tr><tr><td>4670</td><td>0</td><td>3.4624894</td></tr><tr><td>4680</td><td>0</td><td>4.79666</td></tr><tr><td>4690</td><td>0</td><td>5.241359</td></tr><tr><td>4700</td><td>0</td><td>4.539755</td></tr><tr><td>4710</td><td>0</td><td>4.510454</td></tr><tr><td>4720</td><td>0</td><td>3.407005</td></tr><tr><td>7500</td><td>0</td><td>3.532408</td></tr><tr><td>7510</td><td>0</td><td>2.8753278</td></tr><tr><td>7520</td><td>0</td><td>3.1379492</td></tr><tr><td>7530</td><td>0</td><td>4.4375944</td></tr><tr><td>7540</td><td>0</td><td>3.9282098</td></tr><tr><td>7550</td><td>0</td><td>3.1801174</td></tr><tr><td>7560</td><td>0</td><td>3.1342604</td></tr><tr><td>7570</td><td>0</td><td>4.051653</td></tr><tr><td>7580</td><td>0</td><td>1.6253448</td></tr><tr><td>7590</td><td>0</td><td>4.3284984</td></tr><tr><td>7600</td><td>0</td><td>2.058016</td></tr><tr><td>7610</td><td>0</td><td>4.2579236</td></tr><tr><td>7620</td><td>0</td><td>1.3069173</td></tr><tr><td>7630</td><td>0</td><td>3.5847917</td></tr><tr><td>7640</td><td>0</td><td>1.2527022</td></tr><tr><td>7650</td><td>0</td><td>3.3585775</td></tr><tr><td>7660</td><td>0</td><td>5.379416</td></tr><tr><td>7670</td><td>0</td><td>3.1075923</td></tr><tr><td>7680</td><td>0</td><td>5.0969605</td></tr><tr><td>7690</td><td>0</td><td>3.343183</td></tr><tr><td>7700</td><td>0</td><td>3.7997284</td></tr><tr><td>7710</td><td>0</td><td>2.1517313</td></tr><tr><td>7720</td><td>0</td><td>3.7439814</td></tr><tr><td>7730</td><td>0</td><td>3.137449</td></tr><tr><td>7740</td><td>0</td><td>4.648468</td></tr><tr><td>7750</td><td>0</td><td>2.7682767</td></tr><tr><td>7760</td><td>0</td><td>2.02313</td></tr><tr><td>7770</td><td>0</td><td>4.7365546</td></tr><tr><td>1120</td><td>0</td><td>2.0673308</td></tr><tr><td>1130</td><td>0</td><td>2.4457257</td></tr><tr><td>1140</td><td>0</td><td>4.909665</td></tr><tr><td>1150</td><td>0</td><td>1.6715238</td></tr><tr><td>1160</td><td>0</td><td>2.8049102</td></tr><tr><td>1170</td><td>0</td><td>2.9623466</td></tr><tr><td>1180</td><td>0</td><td>4.3524847</td></tr><tr><td>1190</td><td>0</td><td>5.0809693</td></tr><tr><td>1200</td><td>0</td><td>3.5774364</td></tr><tr><td>1210</td><td>0</td><td>4.496276</td></tr><tr><td>1220</td><td>0</td><td>3.9904106</td></tr><tr><td>1230</td><td>0</td><td>2.252646</td></tr><tr><td>1240</td><td>0</td><td>4.5834165</td></tr><tr><td>1250</td><td>0</td><td>2.562584</td></tr><tr><td>1260</td><td>0</td><td>4.3189316</td></tr><tr><td>1270</td><td>0</td><td>3.424365</td></tr><tr><td>1280</td><td>0</td><td>2.1379826</td></tr><tr><td>1290</td><td>0</td><td>3.1377065</td></tr><tr><td>1300</td><td>0</td><td>5.100489</td></tr><tr><td>1310</td><td>0</td><td>1.9100977</td></tr><tr><td>1320</td><td>0</td><td>2.3659465</td></tr><tr><td>1330</td><td>0</td><td>1.9735172</td></tr><tr><td>1340</td><td>0</td><td>4.7110996</td></tr><tr><td>1350</td><td>0</td><td>5.5085845</td></tr><tr><td>1360</td><td>0</td><td>4.401086</td></tr><tr><td>1370</td><td>0</td><td>3.8814154</td></tr><tr><td>1380</td><td>0</td><td>3.166877</td></tr><tr><td>6390</td><td>0</td><td>4.4213457</td></tr><tr><td>6400</td><td>0</td><td>2.5519829</td></tr><tr><td>6410</td><td>0</td><td>3.2595477</td></tr><tr><td>6420</td><td>0</td><td>4.180624</td></tr><tr><td>6430</td><td>0</td><td>4.5659304</td></tr><tr><td>6440</td><td>0</td><td>4.0522046</td></tr><tr><td>6450</td><td>0</td><td>3.7001293</td></tr><tr><td>6460</td><td>0</td><td>3.4718888</td></tr><tr><td>6470</td><td>0</td><td>4.8007884</td></tr><tr><td>6480</td><td>0</td><td>4.954156</td></tr><tr><td>6490</td><td>0</td><td>4.099588</td></tr><tr><td>6500</td><td>0</td><td>3.803875</td></tr><tr><td>6510</td><td>0</td><td>4.006662</td></tr><tr><td>6520</td><td>0</td><td>3.5444257</td></tr><tr><td>6530</td><td>0</td><td>3.7879865</td></tr><tr><td>6540</td><td>0</td><td>2.13004</td></tr><tr><td>6550</td><td>0</td><td>1.4669311</td></tr><tr><td>6560</td><td>0</td><td>3.6007297</td></tr><tr><td>6570</td><td>0</td><td>3.5396976</td></tr><tr><td>6580</td><td>0</td><td>5.072945</td></tr><tr><td>6590</td><td>0</td><td>2.4450033</td></tr><tr><td>6600</td><td>0</td><td>5.129183</td></tr><tr><td>6610</td><td>0</td><td>3.8538632</td></tr><tr><td>6620</td><td>0</td><td>3.9925964</td></tr><tr><td>6630</td><td>0</td><td>2.5528665</td></tr><tr><td>6640</td><td>0</td><td>4.5509834</td></tr><tr><td>6650</td><td>0</td><td>4.24575</td></tr><tr><td>6660</td><td>0</td><td>3.7936473</td></tr><tr><td>8890</td><td>0</td><td>4.355129</td></tr><tr><td>8900</td><td>0</td><td>3.0006795</td></tr><tr><td>8910</td><td>0</td><td>2.2131777</td></tr><tr><td>8920</td><td>0</td><td>2.1140616</td></tr><tr><td>8930</td><td>0</td><td>3.2315483</td></tr><tr><td>8940</td><td>0</td><td>4.027316</td></tr><tr><td>8950</td><td>0</td><td>2.5241177</td></tr><tr><td>8960</td><td>0</td><td>4.391586</td></tr><tr><td>8970</td><td>0</td><td>2.458983</td></tr><tr><td>8980</td><td>0</td><td>3.33078</td></tr><tr><td>8990</td><td>0</td><td>2.8163445</td></tr><tr><td>9000</td><td>0</td><td>3.3174856</td></tr><tr><td>9010</td><td>0</td><td>3.069239</td></tr><tr><td>9020</td><td>0</td><td>3.925005</td></tr><tr><td>9030</td><td>0</td><td>1.3461841</td></tr><tr><td>9040</td><td>0</td><td>3.5994906</td></tr><tr><td>9050</td><td>0</td><td>3.1103637</td></tr><tr><td>9060</td><td>0</td><td>3.843075</td></tr><tr><td>9070</td><td>0</td><td>3.6124077</td></tr><tr><td>9080</td><td>0</td><td>1.7637148</td></tr><tr><td>9090</td><td>0</td><td>4.768984</td></tr><tr><td>9100</td><td>0</td><td>3.0058858</td></tr><tr><td>9110</td><td>0</td><td>4.5664096</td></tr><tr><td>9120</td><td>0</td><td>3.6722593</td></tr><tr><td>9130</td><td>0</td><td>3.0266945</td></tr><tr><td>9140</td><td>0</td><td>4.212329</td></tr><tr><td>9150</td><td>0</td><td>1.6320851</td></tr><tr><td>9160</td><td>0</td><td>2.4228704</td></tr><tr><td>560</td><td>0</td><td>2.6420057</td></tr><tr><td>570</td><td>0</td><td>2.5750735</td></tr><tr><td>580</td><td>0</td><td>4.671599</td></tr><tr><td>590</td><td>0</td><td>3.4082427</td></tr><tr><td>600</td><td>0</td><td>4.3439136</td></tr><tr><td>610</td><td>0</td><td>4.6205206</td></tr><tr><td>620</td><td>0</td><td>1.5307987</td></tr><tr><td>630</td><td>0</td><td>3.075915</td></tr><tr><td>640</td><td>0</td><td>3.9907253</td></tr><tr><td>650</td><td>0</td><td>2.4844778</td></tr><tr><td>660</td><td>0</td><td>3.6065383</td></tr><tr><td>670</td><td>0</td><td>4.558839</td></tr><tr><td>680</td><td>0</td><td>4.2215967</td></tr><tr><td>690</td><td>0</td><td>1.6701895</td></tr><tr><td>700</td><td>0</td><td>3.1688976</td></tr><tr><td>710</td><td>0</td><td>4.972612</td></tr><tr><td>720</td><td>0</td><td>2.7408292</td></tr><tr><td>730</td><td>0</td><td>4.5171356</td></tr><tr><td>740</td><td>0</td><td>1.6784667</td></tr><tr><td>750</td><td>0</td><td>1.5839627</td></tr><tr><td>760</td><td>0</td><td>3.9569018</td></tr><tr><td>770</td><td>0</td><td>4.0115385</td></tr><tr><td>780</td><td>0</td><td>3.9229755</td></tr><tr><td>790</td><td>0</td><td>4.0385695</td></tr><tr><td>800</td><td>0</td><td>4.044871</td></tr><tr><td>810</td><td>0</td><td>3.364149</td></tr><tr><td>820</td><td>0</td><td>4.497357</td></tr><tr><td>830</td><td>0</td><td>2.125903</td></tr><tr><td>1390</td><td>0</td><td>4.864722</td></tr><tr><td>1400</td><td>0</td><td>4.3711224</td></tr><tr><td>1410</td><td>0</td><td>2.6602027</td></tr><tr><td>1420</td><td>0</td><td>2.9229097</td></tr><tr><td>1430</td><td>0</td><td>2.959405</td></tr><tr><td>1440</td><td>0</td><td>3.9056728</td></tr><tr><td>1450</td><td>0</td><td>2.9539695</td></tr><tr><td>1460</td><td>0</td><td>2.141777</td></tr><tr><td>1470</td><td>0</td><td>4.81652</td></tr><tr><td>1480</td><td>0</td><td>2.2646356</td></tr><tr><td>1490</td><td>0</td><td>1.6818937</td></tr><tr><td>1500</td><td>0</td><td>2.6970172</td></tr><tr><td>1510</td><td>0</td><td>3.5531225</td></tr><tr><td>1520</td><td>0</td><td>4.3923373</td></tr><tr><td>1530</td><td>0</td><td>2.731747</td></tr><tr><td>1540</td><td>0</td><td>4.394969</td></tr><tr><td>1550</td><td>0</td><td>1.8018237</td></tr><tr><td>1560</td><td>0</td><td>3.654761</td></tr><tr><td>1570</td><td>0</td><td>4.793758</td></tr><tr><td>1580</td><td>0</td><td>5.8981977</td></tr><tr><td>1590</td><td>0</td><td>4.852207</td></tr><tr><td>1600</td><td>0</td><td>2.0784209</td></tr><tr><td>1610</td><td>0</td><td>5.3256392</td></tr><tr><td>1620</td><td>0</td><td>2.5579975</td></tr><tr><td>1630</td><td>0</td><td>3.540712</td></tr><tr><td>1640</td><td>0</td><td>2.3048968</td></tr><tr><td>1650</td><td>0</td><td>3.6463485</td></tr><tr><td>1660</td><td>0</td><td>3.1124876</td></tr><tr><td>5560</td><td>0</td><td>3.4623833</td></tr><tr><td>5570</td><td>0</td><td>4.2423573</td></tr><tr><td>5580</td><td>0</td><td>3.9420683</td></tr><tr><td>5590</td><td>0</td><td>2.9295106</td></tr><tr><td>5600</td><td>0</td><td>2.633657</td></tr><tr><td>5610</td><td>0</td><td>2.4896815</td></tr><tr><td>5620</td><td>0</td><td>2.4126625</td></tr><tr><td>5630</td><td>0</td><td>2.9739969</td></tr><tr><td>5640</td><td>0</td><td>0.8904342</td></tr><tr><td>5650</td><td>0</td><td>4.5106325</td></tr><tr><td>5660</td><td>0</td><td>2.1702645</td></tr><tr><td>5670</td><td>0</td><td>3.8562863</td></tr><tr><td>5680</td><td>0</td><td>4.106771</td></tr><tr><td>5690</td><td>0</td><td>4.889598</td></tr><tr><td>5700</td><td>0</td><td>1.5519677</td></tr><tr><td>5710</td><td>0</td><td>2.7294643</td></tr><tr><td>5720</td><td>0</td><td>3.3711421</td></tr><tr><td>5730</td><td>0</td><td>3.7641904</td></tr><tr><td>5740</td><td>0</td><td>5.9717827</td></tr><tr><td>5750</td><td>0</td><td>5.539708</td></tr><tr><td>5760</td><td>0</td><td>4.833786</td></tr><tr><td>5770</td><td>0</td><td>2.3599901</td></tr><tr><td>5780</td><td>0</td><td>4.471308</td></tr><tr><td>5790</td><td>0</td><td>2.6966376</td></tr><tr><td>5800</td><td>0</td><td>3.8166254</td></tr><tr><td>5810</td><td>0</td><td>1.9428457</td></tr><tr><td>5820</td><td>0</td><td>2.7672591</td></tr><tr><td>5830</td><td>0</td><td>3.4573224</td></tr><tr><td>5000</td><td>0</td><td>2.162168</td></tr><tr><td>5010</td><td>0</td><td>2.9310286</td></tr><tr><td>5020</td><td>0</td><td>2.4820044</td></tr><tr><td>5030</td><td>0</td><td>2.3671196</td></tr><tr><td>5040</td><td>0</td><td>4.8012166</td></tr><tr><td>5050</td><td>0</td><td>2.6391585</td></tr><tr><td>5060</td><td>0</td><td>4.4827137</td></tr><tr><td>5070</td><td>0</td><td>2.1406884</td></tr><tr><td>5080</td><td>0</td><td>3.6322277</td></tr><tr><td>5090</td><td>0</td><td>4.136498</td></tr><tr><td>5100</td><td>0</td><td>5.6600504</td></tr><tr><td>5110</td><td>0</td><td>3.2250738</td></tr><tr><td>5120</td><td>0</td><td>3.8343565</td></tr><tr><td>5130</td><td>0</td><td>2.9857087</td></tr><tr><td>5140</td><td>0</td><td>4.836108</td></tr><tr><td>5150</td><td>0</td><td>3.9769523</td></tr><tr><td>5160</td><td>0</td><td>3.6813817</td></tr><tr><td>5170</td><td>0</td><td>4.9555945</td></tr><tr><td>5180</td><td>0</td><td>2.716229</td></tr><tr><td>5190</td><td>0</td><td>2.862189</td></tr><tr><td>5200</td><td>0</td><td>4.8586073</td></tr><tr><td>5210</td><td>0</td><td>4.220908</td></tr><tr><td>5220</td><td>0</td><td>2.8558931</td></tr><tr><td>5230</td><td>0</td><td>3.906788</td></tr><tr><td>5240</td><td>0</td><td>3.1385195</td></tr><tr><td>5250</td><td>0</td><td>2.3380015</td></tr><tr><td>5260</td><td>0</td><td>2.491093</td></tr><tr><td>5270</td><td>0</td><td>2.3216805</td></tr><tr><td>5840</td><td>0</td><td>3.5073948</td></tr><tr><td>5850</td><td>0</td><td>3.1504953</td></tr><tr><td>5860</td><td>0</td><td>2.4833138</td></tr><tr><td>5870</td><td>0</td><td>4.194328</td></tr><tr><td>5880</td><td>0</td><td>4.2472167</td></tr><tr><td>5890</td><td>0</td><td>3.1967776</td></tr><tr><td>5900</td><td>0</td><td>2.0377932</td></tr><tr><td>5910</td><td>0</td><td>2.6221578</td></tr><tr><td>5920</td><td>0</td><td>3.225381</td></tr><tr><td>5930</td><td>0</td><td>4.573904</td></tr><tr><td>5940</td><td>0</td><td>4.1289577</td></tr><tr><td>5950</td><td>0</td><td>3.6646695</td></tr><tr><td>5960</td><td>0</td><td>2.7929046</td></tr><tr><td>5970</td><td>0</td><td>5.5715966</td></tr><tr><td>5980</td><td>0</td><td>3.184781</td></tr><tr><td>5990</td><td>0</td><td>4.7297173</td></tr><tr><td>6000</td><td>0</td><td>2.4762774</td></tr><tr><td>6010</td><td>0</td><td>2.593985</td></tr><tr><td>6020</td><td>0</td><td>2.5157416</td></tr><tr><td>6030</td><td>0</td><td>4.7940817</td></tr><tr><td>6040</td><td>0</td><td>4.8454356</td></tr><tr><td>6050</td><td>0</td><td>3.4565563</td></tr><tr><td>6060</td><td>0</td><td>5.875077</td></tr><tr><td>6070</td><td>0</td><td>2.3522913</td></tr><tr><td>6080</td><td>0</td><td>5.1749415</td></tr><tr><td>6090</td><td>0</td><td>2.7705884</td></tr><tr><td>6100</td><td>0</td><td>2.2337267</td></tr><tr><td>6110</td><td>0</td><td>1.8446498</td></tr><tr><td>9450</td><td>0</td><td>3.2620358</td></tr><tr><td>9460</td><td>0</td><td>4.854634</td></tr><tr><td>9470</td><td>0</td><td>5.0100374</td></tr><tr><td>9480</td><td>0</td><td>3.868127</td></tr><tr><td>9490</td><td>0</td><td>5.0941525</td></tr><tr><td>9500</td><td>0</td><td>3.5293193</td></tr><tr><td>9510</td><td>0</td><td>2.6718798</td></tr><tr><td>9520</td><td>0</td><td>4.54468</td></tr><tr><td>9530</td><td>0</td><td>2.541386</td></tr><tr><td>9540</td><td>0</td><td>3.1260977</td></tr><tr><td>9550</td><td>0</td><td>2.3481712</td></tr><tr><td>9560</td><td>0</td><td>2.2238848</td></tr><tr><td>9570</td><td>0</td><td>4.325117</td></tr><tr><td>9580</td><td>0</td><td>5.175755</td></tr><tr><td>9590</td><td>0</td><td>2.6275406</td></tr><tr><td>9600</td><td>0</td><td>5.6835384</td></tr><tr><td>9610</td><td>0</td><td>2.565141</td></tr><tr><td>9620</td><td>0</td><td>3.255987</td></tr><tr><td>9630</td><td>0</td><td>2.9154127</td></tr><tr><td>9640</td><td>0</td><td>1.6452045</td></tr><tr><td>9650</td><td>0</td><td>4.0490856</td></tr><tr><td>9660</td><td>0</td><td>4.1099834</td></tr><tr><td>9670</td><td>0</td><td>4.1809034</td></tr><tr><td>9680</td><td>0</td><td>4.863692</td></tr><tr><td>9690</td><td>0</td><td>4.788409</td></tr><tr><td>9700</td><td>0</td><td>3.3809812</td></tr><tr><td>9710</td><td>0</td><td>4.6966577</td></tr><tr><td>9720</td><td>0</td><td>2.5549989</td></tr></tbody></table></div>"]}}]}, {"metadata": {}, "cell_type": "markdown", "source": "You made it to the end!  Here's a picture of a unicorn:\n\n![unicorn](https://www.jing.fm/clipimg/detail/37-375094_galaxy-unicorn-png.png)"}], "nbformat_minor": 4}